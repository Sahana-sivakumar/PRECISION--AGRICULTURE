# -*- coding: utf-8 -*-
"""SVM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWvYgwKToy6XM_AGFrMuSTiHbroKcV7r

Imports the pandas library as pd:
"""

import pandas as pd
df = pd.read_csv("/content/sample_data/Disease with Weather dataset.csv")
df

"""Imports necessary modules for data scaling:"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import numpy as np
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
print("Original data: \n", df)
print("\nScaled data: \n",scaled_data)

"""MinMaxScaler object:"""

scaler1 = MinMaxScaler()
scaled_data1 = scaler1.fit_transform(df)
print("Original data: \n", df)
print("\nScaled data: \n",scaled_data1)

"""Instantiates a RobustScaler object:"""

scaler2 = RobustScaler()
scaled_data2 = scaler1.fit_transform(df)
print("Original data: \n", df)
print("\nScaled data: \n",scaled_data2)

"""normalized the scaled data:"""

from sklearn.preprocessing import normalize
normalized_scaled_data = normalize(scaled_data)
normalized_scaled_data1 = normalize(scaled_data1)
normalized_scaled_data2 = normalize(scaled_data2)
print("Normalised scaled data:\n ", normalized_scaled_data)
print("Normalised scaled data1:\n ", normalized_scaled_data1)
print("Normalised scaled data2:\n", normalized_scaled_data2)

"""Imports pandas and train_test_split"""

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/sample_data/Disease with Weather dataset.csv')  # Replace with your dataset file
X = df.iloc[:, :-1]  # Features (input variables)
y = df.iloc[:, -1]   # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)

X_train, X_test

import pandas as pd

# Read data from file (adjust the file path accordingly)
data = pd.read_csv(r"/content/sample_data/Disease with Weather dataset.csv")

# Preview the first 5 rows of the loaded data
print(data.head())

"""use PCA and find dimensions:"""

# Import necessary libraries
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the breast_cancer dataset from sklearn.datasets
data_file ='/content/sample_data/Disease with Weather dataset.csv'
df = pd.read_csv(data_file)

# Drop non-numeric columns if any
# Assuming you only want to perform PCA on numeric data
numeric_df = df.select_dtypes(include=[np.number])

# Standardize the dataset (essential before applying PCA)
scaler = StandardScaler()
X = scaler.fit_transform(numeric_df)

# Initialize PCA with the desired number of components (e.g., 3)
pca = PCA(n_components=3)

# Fit and transform the data
X_pca = pca.fit_transform(X)

# Check the dimensions of X_pca
print("Dimensions of X_pca:", X_pca.shape)

from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Choose a classification algorithm
clf = SVC(kernel='linear', random_state=42)

# Train the classification model on the PCA-transformed training data
clf.fit(X_train_pca, y_train)

# Predict on the PCA-transformed testing data
y_pred = clf.predict(X_test_pca)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Choose a classification algorithm
clf = SVC(kernel='linear', random_state=42)

# Train the classification model on the PCA-transformed training data
clf.fit(X_train_pca, y_train)

# Predict on the PCA-transformed testing data
y_pred = clf.predict(X_test_pca)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)

# Choose a classification algorithm
clf = SVC(kernel='linear', random_state=42)

# Train the classification model on the PCA-transformed training data
clf.fit(X_train_pca, y_train)

# Apply PCA to the test data
X_test_pca = pca.transform(X_test_scaled)

# Define the meshgrid
x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1
y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

# Plot decision boundary
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, s=20, edgecolor='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Decision Boundary Visualization')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Generate random data for temperature, soil moisture, humidity, and classes
np.random.seed(0)
num_samples = 150
temperature = np.random.normal(20, 5, num_samples)  # mean=20, std=5
soil_moisture = np.random.normal(50, 10, num_samples)  # mean=50, std=10
humidity = np.random.normal(60, 15, num_samples)  # mean=60, std=15
classes = iris.target

# Create scatter plot
plt.figure(figsize=(10, 6))

# Plot each class separately
for i in range(3):
    plt.scatter(temperature[classes == i], soil_moisture[classes == i], label=f'Iris-{i}', alpha=0.8)

plt.xlabel('Temperature')
plt.ylabel('Soil Moisture')
plt.title('Relationship between Temperature and Soil Moisture with Iris Classes')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Generate synthetic data
X, y = make_classification(n_samples=100, n_features=3, n_classes=3, n_clusters_per_class=1,
                           n_redundant=0, n_informative=3, random_state=42)

# Extract features
temperature = X[:, 0]
soil_moisture = X[:, 1]
humidity = X[:, 2]

# Create scatter plot
plt.figure(figsize=(10, 6))

# Plot each class separately
for i in range(3):
    plt.scatter(temperature[y == i], soil_moisture[y == i], label=f'Class {i}', alpha=0.8)

plt.xlabel('Temperature')
plt.ylabel('Soil Moisture')
plt.title('Relationship between Temperature and Soil Moisture with Class Labels')
plt.legend()
plt.grid(True)
plt.show()