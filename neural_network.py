# -*- coding: utf-8 -*-
"""neural network

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jfDjWLuE4Z4rNINPi7MQyd5fgb5sNet0
"""



"""import pandas:"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

url = "/Disease with Weather dataset final.csv"
my_df = pd.read_csv(url)

my_df

my_df['variety'] = my_df['variety'].replace('ginger', 0.0)
my_df

X = my_df.drop('variety', axis=1)
y = my_df['variety']

X = X.values
y = y.values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)

import torch

X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)

import torch

y_train = torch.LongTensor(y_train)
y_test = torch.LongTensor(y_test)

import torch
import torch.nn as nn
import torch.optim as optim

# Assuming you have defined your neural network class (replace 'YourNeuralNetwork' with your actual class name)
class myNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(myNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Assuming your input, hidden, and output sizes are known
input_size = 100  # Replace with the actual input size
hidden_size = 50   # Replace with the actual hidden size
output_size = 10   # Replace with the actual output size

# Instantiate your model
model = myNeuralNetwork(input_size, hidden_size, output_size)

# Set up the loss criterion and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

model.parameters

"""Neural Network Forward Pass:"""

class myNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(myNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

class myNeuralNetwork(nn.Module):
    def __init__(self):
        # ...
        self.fc1 = nn.Linear(100, 50)  # Example input size (incorrect)
        # ...

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Assuming X_train and y_train are your training data
# Also assuming YourNeuralNetwork is your model class

# Sample Data (replace with your actual data)
X_train = torch.rand((100, 3))
y_train = torch.randint(0, 2, (100,)).long()

# Model Definition (replace with your actual model)
class myNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(myNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Model, Loss Function, and Optimizer
input_size = 3  # Adjust based on your actual input size
hidden_size = 50  # Adjust based on your hidden layer size
output_size = 2  # Binary classification, so output size is 2
model = myNeuralNetwork(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training Loop
epochs = 100
losses = []

for i in range(epochs):
    # Forward pass
    y_pred = model(X_train)

    # Calculate loss
    loss = criterion(y_pred, y_train)
    losses.append(loss.item())

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print loss every 10 epochs
    if i % 10 == 0:
        print(f'Epoch: {i}, Loss: {loss.item()}')

# Plot the losses
plt.plot(range(epochs), losses)
plt.ylabel("Loss/Error")
plt.xlabel("Epochs")
plt.title("Training Loss over Epochs")
plt.show()

import torch
import torch.nn as nn

# Define your neural network model
class myModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(myModel, self).__init__()
        # Define your model architecture here
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        # Define the forward pass of your model
        return self.linear(x)

# Initialize an instance of your model
input_size = 20
output_size = 20
model = myModel(input_size, output_size)

# Load or define your test data (X_test)
# For demonstration purposes, assuming X_test is a tensor with shape (batch_size, input_size)
X_test = torch.randn(10, input_size)  # Replace this with your actual test data

# Assuming you have other necessary imports and data loading here

# Now you can use the model within the torch.no_grad() block
with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    y_eval = model.forward(X_test)

    # Assuming y_test is ground truth (replace with your actual ground truth)
    y_test = torch.randn(10, output_size)

    # Define your loss function (criterion)
    criterion = nn.MSELoss()  # For example, using Mean Squared Error loss

    # Calculate the loss
    loss = criterion(y_eval, y_test)

print("Loss:", loss.item())

correct = 0

with torch.no_grad():
    for i, data in enumerate(X_test):
        y_val = model.forward(data)

        # Assuming y_test is ground truth (replace with your actual ground truth)
        # Also, make sure y_test is a tensor
        if (y_test[i] == 0).all():
            x = "ginger"
            correct += 1

        print(f'{i+1}. {str(y_val)}')

print(f'We got {correct} correct!')

import torch

new_iris = torch.tensor([18.7, 77, 33.51])

import torch
import torch.nn as nn

# Define a simple model for illustration purposes
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(3, 1)  # Assuming input size is 3 for newer_iris

    def forward(self, x):
        return self.fc(x)

# Create an instance of the model
model = SimpleModel()

# Now you can use the model with torch.no_grad()
newer_iris = torch.tensor([18.7, 77, 33.51])
with torch.no_grad():
    output = model(newer_iris)
    print(output)

import torch

new_iris = torch.tensor([21.0,	73,	29.75])

import torch
import torch.nn as nn

# Define a simple model for illustration purposes
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(3, 1)  # Assuming input size is 3 for newer_iris

    def forward(self, x):
        return self.fc(x)

# Create an instance of the model
model = SimpleModel()

# Now you can use the model with torch.no_grad()
newer_iris = torch.tensor([21.0,	73,	29.75])
with torch.no_grad():
    output = model(newer_iris)
    print(output)