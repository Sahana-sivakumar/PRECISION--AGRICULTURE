# -*- coding: utf-8 -*-
"""Enhanced Random Forest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wMvYW5S8ogdcrpE0N3ibCLS3Clu-ZEIw
"""

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

import pandas as pd
df = pd.read_csv("/content/sample_data/Disease with Weather dataset (1).csv")
df

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Assuming 'df' is your DataFrame
# Exclude non-numeric columns
numeric_columns = df.select_dtypes(include=[np.number]).columns

# Select only the numeric columns for scaling
numeric_data = df[numeric_columns]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the scaler on numeric data
scaled_data = scaler.fit_transform(numeric_data)

# Replace the original numeric columns with the scaled data
df[numeric_columns] = scaled_data

# Display the original and scaled data
print("Original data: \n", df)

# Assuming 'df' is your DataFrame
non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns
df_numeric = df.drop(non_numeric_columns, axis=1)  # Assuming you want to drop non-numeric columns

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df_numeric)

# Assuming 'df' is your DataFrame
non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns
df_numeric = df.drop(non_numeric_columns, axis=1)  # Assuming you want to drop non-numeric columns

from sklearn.preprocessing import RobustScaler

scaler2 = RobustScaler()
scaled_data2 = scaler2.fit_transform(df_numeric)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a DataFrame 'df' with features and labels
# Make sure to adjust the column names accordingly

# Specify the correct target column name
target_column = 'target_column'  # Replace with the actual name of your target column

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    # Separate features (X) and labels (y)
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Now you can use train_test_split
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize the Random Forest Classifier
    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

    # Train the Random Forest model
    rf_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = rf_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
else:
    print(f"Error: '{target_column}' not found in the DataFrame.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming your dataset is in a DataFrame
# You may load it from a CSV file or create a DataFrame in another way
# For this example, let's assume the dataset is in a file named 'your_dataset.csv'
df = pd.read_csv('/content/sample_data/Disease with Weather dataset (1).csv')

# Separate features (X) and labels (y)
X = df.drop('variety', axis=1)
y = df['variety']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a DataFrame named 'data' with features and labels
# For example, X represents features, and y represents labels
# X = data[['feature1', 'feature2', 'feature3', ...]]
# y = data['label']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Number of random forest models in the ensemble
n_estimators = 10

# Initialize an ensemble of random forest classifiers
ensemble = [RandomForestClassifier() for _ in range(n_estimators)]

# Train each random forest in the ensemble on different subsets of the training data
for i in range(n_estimators):
    # Assume each model is trained on a different subset of the training data
    ensemble[i].fit(X_train, y_train)

# Make predictions on the testing data using each model in the ensemble
predictions = [model.predict(X_test) for model in ensemble]

# Use majority voting to determine the final predictions of the ensemble
final_predictions = [max(set(prediction), key=prediction.count) for prediction in zip(*predictions)]

# Calculate the accuracy of the ensemble on the testing data
accuracy = accuracy_score(y_test, final_predictions)

print(f"Ensemble Accuracy: {accuracy}")

import matplotlib.pyplot as plt
import numpy as np

# Example: Replace this list with the actual accuracies for each model in your ensemble
accuracies = [0.85, 0.88, 0.92, 0.89, 0.91, 0.87, 0.90, 0.93, 0.86, 0.88]

# Create a list of model indices for the x-axis
model_indices = np.arange(1, len(accuracies) + 1)

# Plotting the bar graph
plt.bar(model_indices, accuracies, color='blue', alpha=0.7)

# Adding labels and title
plt.xlabel('Model Index')
plt.ylabel('Accuracy')
plt.title('Accuracy of Each Model in the Ensemble')

# Adding individual accuracy values on top of the bars
for i, acc in enumerate(accuracies):
    plt.text(model_indices[i], acc + 0.01, f'{acc:.2f}', ha='center', va='bottom')

# Show the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming your dataset is in a DataFrame
# You may load it from a CSV file or create a DataFrame in another way
# For this example, let's assume the dataset is in a file named 'your_dataset.csv'
df = pd.read_csv('/content/sample_data/Disease with Weather dataset (1).csv')

# Separate features (X) and labels (y)
X = df.drop('variety', axis=1)
y = df['variety']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Get feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Get feature names from the original dataset
feature_names = X.columns

# Sort indices based on feature importances
sorted_indices = np.argsort(feature_importances)

# Plotting the bar graph
plt.figure(figsize=(10, 6))
plt.barh(range(len(feature_importances)), feature_importances[sorted_indices], align='center', color='skyblue')
plt.yticks(range(len(feature_importances)), [feature_names[i] for i in sorted_indices])
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Random Forest Feature Importance')

# Adding a horizontal line to indicate the threshold for displaying feature names
plt.axvline(x=0.02, color='red', linestyle='--', linewidth=2)

plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained the Random Forest model and evaluated the accuracy
accuracy = accuracy_score(y_test, y_pred)

# Get feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Get feature names from the original dataset
feature_names = X.columns

# Sort indices based on feature importances
sorted_indices = np.argsort(feature_importances)

# Plotting the bar graph for feature importance
plt.figure(figsize=(12, 6))

# Subplot for feature importance
plt.subplot(1, 2, 1)
plt.barh(range(len(feature_importances)), feature_importances[sorted_indices], align='center', color='skyblue')
plt.yticks(range(len(feature_importances)), [feature_names[i] for i in sorted_indices])
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Random Forest Feature Importance')

# Subplot for accuracy
plt.subplot(1, 2, 2)
plt.bar([1], [accuracy], color='green', alpha=0.7)
plt.xticks([1], ['Accuracy'])
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.title('Model Accuracy')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create a range of values for the number of estimators
estimator_values = list(range(1, 11))

# Initialize an empty list to store accuracy values
accuracy_values = []

# Iterate over different numbers of estimators and calculate accuracy for each ensemble
for n_estimators in estimator_values:
    # Initialize an ensemble of random forest classifiers
    ensemble = [RandomForestClassifier() for _ in range(n_estimators)]

    # Train each random forest in the ensemble on different subsets of the training data
    for i in range(n_estimators):
        ensemble[i].fit(X_train, y_train)

    # Make predictions on the testing data using each model in the ensemble
    predictions = [model.predict(X_test) for model in ensemble]

    # Use majority voting to determine the final predictions of the ensemble
    final_predictions = [max(set(prediction), key=prediction.count) for prediction in zip(*predictions)]

    # Calculate the accuracy of the ensemble on the testing data
    accuracy = accuracy_score(y_test, final_predictions)

    # Append the accuracy to the list
    accuracy_values.append(accuracy)

# Plot the accuracy values against the number of estimators
plt.plot(estimator_values, accuracy_values, marker='o')
plt.title('Ensemble Accuracy vs Number of Estimators')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/sample_data/Disease with Weather dataset final.csv')

# Separate features (X) and labels (y)
X = df.drop('variety', axis=1)
y = df['variety']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Define a list to store accuracy scores
accuracy_scores = []

# Define a range of values for the number of estimators (trees)
num_estimators_values = [10, 50, 100, 150, 200]

# Loop through different values of number of estimators
for num_estimators in num_estimators_values:
    # Initialize the Random Forest Classifier
    rf_classifier = RandomForestClassifier(n_estimators=num_estimators, random_state=42)

    # Train the Random Forest model
    rf_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = rf_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)

    # Append accuracy score to the list
    accuracy_scores.append(accuracy)

# Plot accuracy scores
plt.figure(figsize=(8, 6))
plt.plot(num_estimators_values, accuracy_scores, marker='o', linestyle='-')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Number of Estimators')
plt.grid(True)
plt.show()